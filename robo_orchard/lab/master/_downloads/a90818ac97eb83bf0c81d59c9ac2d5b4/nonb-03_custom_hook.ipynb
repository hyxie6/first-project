{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# (Advanced) Creating Custom Hooks for Tailored Logic\nIn many complex training scenarios, you might need to inject custom logic at various\npoints within the training loop (e.g., at the beginning/end of an epoch, or before/after\na training step). The ``robo_orchard_lab`` framework provides a powerful and flexible\nHook system based on ``PipelineHooks`` to achieve this without modifying the core\ntraining engine.\n\nThis tutorial will guide you through:\n1. Understanding the core components: ``PipelineHooksConfig``, ``PipelineHooks``, ``HookContext``, and ``PipelineHookArgs``.\n2. Creating a custom hook class that bundles logging logic for different training stages.\n3. Configuring and instantiating your custom hook.\n4. (Simulated) Seeing how this hook would interact with a training engine.\n\nLet's get started!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reuse the code from previous tutorial\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\nimport os\nfrom typing import Any, Optional, Tuple\n\nimport torch\n\nfrom robo_orchard_lab.utils import log_basic_config\n\nlog_basic_config(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nfrom pydantic import Field\nfrom robo_orchard_core.utils.cli import SettingConfig\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets, models, transforms\n\n\nclass DatasetConfig(SettingConfig):\n    \"\"\"Configuration for the dataset.\n\n    This is a example configuration for the ImageNet dataset.\n    \"\"\"\n\n    data_root: Optional[str] = Field(\n        description=\"Image dataset directory.\", default=None\n    )\n\n    pipeline_test: bool = Field(\n        description=\"Whether or not use dummy data for fast pipeline test.\",\n        default=False,\n    )\n\n    dummy_train_imgs: int = Field(\n        description=\"Number of dummy training images.\",\n        default=1024,\n    )\n\n    dummy_val_imgs: int = Field(\n        description=\"Number of dummy validation images.\",\n        default=256,\n    )\n\n    def __post_init__(self):\n        if self.pipeline_test is False and self.data_root is None:\n            raise ValueError(\n                \"data_root must be specified when pipeline_test is False.\"\n            )\n\n    def get_dataset(self) -> Tuple[Dataset, Dataset]:\n        if self.pipeline_test:\n            train_dataset = datasets.FakeData(\n                self.dummy_train_imgs,\n                (3, 224, 224),\n                1000,\n                transforms.ToTensor(),\n            )\n            val_dataset = datasets.FakeData(\n                self.dummy_val_imgs, (3, 224, 224), 1000, transforms.ToTensor()\n            )\n        else:\n            assert self.data_root is not None\n            train_dataset = datasets.ImageFolder(\n                os.path.join(self.data_root, \"train\"),\n                transform=transforms.Compose(\n                    [\n                        transforms.RandomResizedCrop(224),\n                        transforms.RandomHorizontalFlip(),\n                        transforms.ToTensor(),\n                        transforms.Normalize(\n                            mean=[0.485, 0.456, 0.406],\n                            std=[0.229, 0.224, 0.225],\n                        ),\n                    ]\n                ),\n            )\n\n            val_dataset = datasets.ImageFolder(\n                os.path.join(self.data_root, \"val\"),\n                transform=transforms.Compose(\n                    [\n                        transforms.Resize(256),\n                        transforms.CenterCrop(224),\n                        transforms.ToTensor(),\n                        transforms.Normalize(\n                            mean=[0.485, 0.456, 0.406],\n                            std=[0.229, 0.224, 0.225],\n                        ),\n                    ]\n                ),\n            )\n        return train_dataset, val_dataset\n\n\nclass TrainerConfig(SettingConfig):\n    \"\"\"Configuration for the trainer.\n\n    This is an example configuration for training a ResNet50 model\n    on ImageNet. Only a few parameters are set here for demonstration\n    purposes.\n    \"\"\"\n\n    dataset: DatasetConfig = Field(\n        description=\"Dataset configuration. Need to be set by user.\",\n    )\n\n    batch_size: int = Field(\n        description=\"Batch size for training.\",\n        default=128,\n    )\n\n    num_workers: int = Field(\n        description=\"Number of workers for data loading.\",\n        default=4,\n    )\n\n    max_epoch: int = Field(\n        description=\"Maximum number of epochs for training.\",\n        default=90,\n    )\n\n    workspace_root: str = Field(\n        description=\"Workspace root directory.\",\n        default=\"./workspace/\",\n    )\n\n\ncfg = TrainerConfig(\n    dataset=DatasetConfig(pipeline_test=True), max_epoch=5, num_workers=0,\n    workspace_root=\"./workspace/tutorial3/\"\n)\n\nfrom accelerate import Accelerator\nfrom accelerate.utils import ProjectConfiguration\n\naccelerator = Accelerator(\n    project_config=ProjectConfiguration(\n        project_dir=cfg.workspace_root,\n        logging_dir=os.path.join(cfg.workspace_root, \"logs\"),\n        automatic_checkpoint_naming=True,\n        total_limit=32,  # Max checkpoints to keep\n    )\n)\n\n\nfrom robo_orchard_lab.pipeline.batch_processor import SimpleBatchProcessor\n\n\nclass MyBatchProcessor(SimpleBatchProcessor):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.criterion = torch.nn.CrossEntropyLoss()  # Define your loss\n\n    def forward(\n        self,\n        model: torch.nn.Module,\n        batch: Tuple[torch.Tensor, torch.Tensor],\n    ) -> Tuple[Any, Optional[torch.Tensor]]:\n        # unpack batch by yourself\n        images, target = batch\n\n        # Accelerator has already moved batch to the correct device\n        output = model(images)\n        loss = self.criterion(output, target) if self.need_backward else None\n\n        return output, loss  # Returns model output and loss\n\n\nfrom torch.optim import SGD\nfrom torch.optim.lr_scheduler import StepLR\n\ntrain_dataset, _ = cfg.dataset.get_dataset()\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=cfg.batch_size,\n    shuffle=True,\n    num_workers=cfg.num_workers,\n    pin_memory=False,\n)\n\nmodel = models.resnet50()\n\noptimizer = SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\nlr_scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n\nhooks = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementing a Custom Hook\n\nImplementing a custom hook in ``robo_orchard_lab`` typically involves two key parts:\n\n1.  **The Hook Implementation Class (e.g., ``MyHook``)**: This is a Python class\n    that inherits from ``robo_orchard_lab.pipeline.hooks.mixin.PipelineHooks``.\n    It contains the actual logic that will be executed at different points (channels)\n    in the training/evaluation pipeline. In its ``__init__`` method, it registers\n    its methods (or other callables) to specific channels like \"on_loop\",\n    \"on_epoch\", or \"on_step\" using ``self.register_hook()``, often with the\n    help of ``HookContext.from_callable()``.\n\n2.  **The Hook Configuration Class (e.g., ``MyHookConfig``)**: This is a Pydantic\n    class that inherits from ``robo_orchard_lab.pipeline.hooks.mixin.PipelineHooksConfig``.\n    Its primary roles are:\n\n    * To specify which Hook Implementation class should be instantiated (typically via a ``class_type`` attribute).\n\n    * To define and validate any parameters that the Hook Implementation needs (e.g., logging frequencies, file paths, thresholds).\n\n**The Relationship and Benefits**:\n\nThis separation of implementation (logic) from configuration (parameters) is a\ncore design principle that offers several advantages:\n\n* **Configurability & Reusability**: The same hook implementation (e.g., ``MyHook``) can be reused in different experiments with different behaviors simply by\n  providing different configurations (e.g., changing ``log_step_freq`` in ``MyHookConfig``). You don't need to change the Python code of the hook itself.\n\n* **Clarity & Maintainability**: The hook's logic is cleanly encapsulated in its\n  class, while its parameters are explicitly defined and validated by its\n  Pydantic config class.\n\n* **Type Safety**: Pydantic ensures that the parameters passed to your hook are of the correct type and meet any defined validation criteria.\n\n* **Integration with the Framework**: When you instantiate the configuration class\n  (e.g., ``my_hook_cfg = MyHookConfig()``), you get an object that knows how to\n  create the actual hook instance (e.g., by calling ``my_hook_cfg()``, it can\n  instantiate ``MyHook`` and pass itself as the ``cfg`` argument to ``MyHook``'s\n  ``__init__``). This allows the framework to manage hooks declaratively through\n  larger configuration files.\n\nIn the following subsections, we will first define the implementation for ``MyHook``\nand then its corresponding configuration class ``MyHookConfig``.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining Your Custom Hook Implementation\nLet's define ``MyHook``. It inherits from ``PipelineHooks``.\nIn its ``__init__`` method, it takes its configuration (``MyHookConfig``)\nand registers its own internal methods as callbacks to different channels\nusing ``self.register_hook()`` and ``HookContext.from_callable()``.\n\n``HookContext.from_callable(before=..., after=...)`` is a convenient way to create\na ``HookContext`` object where its ``on_enter`` method will call the ``before``\nfunction, and its ``on_exit`` method will call the ``after`` function.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from robo_orchard_lab.pipeline.hooks.mixin import (\n    HookContext,\n    PipelineHookArgs,\n    PipelineHooks,\n    PipelineHooksConfig,\n)\n\n\nclass MyHook(PipelineHooks):\n    \"\"\"A custom hook that logs messages at the beginning and end of loops, epochs, and steps, based on configured frequencies.\"\"\"\n\n    def __init__(self, cfg: \"MyHookConfig\"):\n        super().__init__()\n        self.cfg = cfg\n\n        # Register loop-level hooks\n        self.register_hook(\n            channel=\"on_loop\",\n            hook=HookContext.from_callable(\n                before=self._on_loop_begin, after=self._on_loop_end\n            ),\n        )\n\n        # Register step-level hooks\n        self.register_hook(\n            channel=\"on_step\",\n            hook=HookContext.from_callable(\n                before=self._on_step_begin, after=self._on_step_end\n            ),\n        )\n\n        # Register epoch-level hooks\n        self.register_hook(\n            channel=\"on_epoch\",\n            hook=HookContext.from_callable(\n                before=self._on_epoch_begin, after=self._on_epoch_end\n            ),\n        )\n\n        logger.info(\n            f\"MyHook instance created with step_freq={self.cfg.log_step_freq}, epoch_freq={self.cfg.log_epoch_freq}\"\n        )\n\n    def _on_loop_begin(self, args: PipelineHookArgs):\n        logger.info(\"Begining loop\")\n\n    def _on_loop_end(self, args: PipelineHookArgs):\n        logger.info(\"Ended loop\")\n\n    def _on_step_begin(self, args: PipelineHookArgs):\n        # Note: step_id is 0-indexed. Adding 1 for 1-indexed frequency check.\n        if (args.step_id + 1) % self.cfg.log_step_freq == 0:\n            logger.info(\"Begining {}-th step\".format(args.step_id))\n\n    def _on_step_end(self, args: PipelineHookArgs):\n        if (args.step_id + 1) % self.cfg.log_step_freq == 0:\n            logger.info(\"Ended {}-th step\".format(args.step_id))\n\n    def _on_epoch_begin(self, args: PipelineHookArgs):\n        # Note: epoch_id is 0-indexed. Adding 1 for 1-indexed frequency check.\n        if (args.epoch_id + 1) % self.cfg.log_epoch_freq == 0:\n            logger.info(\"Begining {}-th epoch\".format(args.epoch_id))\n\n    def _on_epoch_end(self, args: PipelineHookArgs):\n        if (args.epoch_id + 1) % self.cfg.log_epoch_freq == 0:\n            logger.info(\"Ended {}-th epoch\".format(args.epoch_id))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining Your Custom Hook Configuration\n\nThen, we define a Pydantic configuration class for our custom hook.\nThis class will inherit from ``PipelineHooksConfig`` and specify our custom hook\nclass as its ``class_type``. It will also hold any parameters our hook needs,\nlike logging frequencies.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MyHookConfig(PipelineHooksConfig[MyHook]):\n    class_type: type[MyHook] = MyHook\n    log_step_freq: int = 5\n    log_epoch_freq: int = 1\n\n\nmy_hook = MyHookConfig()\n\nhooks.append(my_hook)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Orchestrating the Training\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from robo_orchard_lab.pipeline import HookBasedTrainer\n\ntrainer = HookBasedTrainer(\n    model=model,\n    dataloader=train_dataloader,\n    optimizer=optimizer,\n    lr_scheduler=lr_scheduler,\n    accelerator=accelerator,\n    batch_processor=MyBatchProcessor(need_backward=True),\n    max_epoch=cfg.max_epoch,\n    hooks=hooks,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Show hooks\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(trainer.hooks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Begin training\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}