{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Leveraging Built-in Hooks for Common Training Tasks\n\nHooks are the primary way to add custom behavior to the training or evaluation loop without modifying its source code.\nThey are called at specific points during training or evaluation (e.g., end of epoch, after a step).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reuse the code from previous tutorial\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\nimport os\nfrom typing import Any, Optional, Tuple\n\nimport torch\n\nfrom robo_orchard_lab.utils import log_basic_config\n\nlog_basic_config(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nfrom pydantic import Field\nfrom robo_orchard_core.utils.cli import SettingConfig\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets, models, transforms\n\n\nclass DatasetConfig(SettingConfig):\n    \"\"\"Configuration for the dataset.\n\n    This is a example configuration for the ImageNet dataset.\n    \"\"\"\n\n    data_root: Optional[str] = Field(\n        description=\"Image dataset directory.\", default=None\n    )\n\n    pipeline_test: bool = Field(\n        description=\"Whether or not use dummy data for fast pipeline test.\",\n        default=False,\n    )\n\n    dummy_train_imgs: int = Field(\n        description=\"Number of dummy training images.\",\n        default=1024,\n    )\n\n    dummy_val_imgs: int = Field(\n        description=\"Number of dummy validation images.\",\n        default=256,\n    )\n\n    def __post_init__(self):\n        if self.pipeline_test is False and self.data_root is None:\n            raise ValueError(\n                \"data_root must be specified when pipeline_test is False.\"\n            )\n\n    def get_dataset(self) -> Tuple[Dataset, Dataset]:\n        if self.pipeline_test:\n            train_dataset = datasets.FakeData(\n                self.dummy_train_imgs,\n                (3, 224, 224),\n                1000,\n                transforms.ToTensor(),\n            )\n            val_dataset = datasets.FakeData(\n                self.dummy_val_imgs, (3, 224, 224), 1000, transforms.ToTensor()\n            )\n        else:\n            assert self.data_root is not None\n            train_dataset = datasets.ImageFolder(\n                os.path.join(self.data_root, \"train\"),\n                transform=transforms.Compose(\n                    [\n                        transforms.RandomResizedCrop(224),\n                        transforms.RandomHorizontalFlip(),\n                        transforms.ToTensor(),\n                        transforms.Normalize(\n                            mean=[0.485, 0.456, 0.406],\n                            std=[0.229, 0.224, 0.225],\n                        ),\n                    ]\n                ),\n            )\n\n            val_dataset = datasets.ImageFolder(\n                os.path.join(self.data_root, \"val\"),\n                transform=transforms.Compose(\n                    [\n                        transforms.Resize(256),\n                        transforms.CenterCrop(224),\n                        transforms.ToTensor(),\n                        transforms.Normalize(\n                            mean=[0.485, 0.456, 0.406],\n                            std=[0.229, 0.224, 0.225],\n                        ),\n                    ]\n                ),\n            )\n        return train_dataset, val_dataset\n\n\nclass TrainerConfig(SettingConfig):\n    \"\"\"Configuration for the trainer.\n\n    This is an example configuration for training a ResNet50 model\n    on ImageNet. Only a few parameters are set here for demonstration\n    purposes.\n    \"\"\"\n\n    dataset: DatasetConfig = Field(\n        description=\"Dataset configuration. Need to be set by user.\",\n    )\n\n    batch_size: int = Field(\n        description=\"Batch size for training.\",\n        default=128,\n    )\n\n    num_workers: int = Field(\n        description=\"Number of workers for data loading.\",\n        default=4,\n    )\n\n    max_epoch: int = Field(\n        description=\"Maximum number of epochs for training.\",\n        default=90,\n    )\n\n    workspace_root: str = Field(\n        description=\"Workspace root directory.\",\n        default=\"./workspace/\",\n    )\n\n\ncfg = TrainerConfig(\n    dataset=DatasetConfig(pipeline_test=True), max_epoch=5, num_workers=0,\n    workspace_root=\"./workspace/tutorial2/\"\n)\n\nfrom accelerate import Accelerator\nfrom accelerate.utils import ProjectConfiguration\n\naccelerator = Accelerator(\n    project_config=ProjectConfiguration(\n        project_dir=cfg.workspace_root,\n        logging_dir=os.path.join(cfg.workspace_root, \"logs\"),\n        automatic_checkpoint_naming=True,\n        total_limit=32,  # Max checkpoints to keep\n    )\n)\n\n\nfrom robo_orchard_lab.pipeline.batch_processor import SimpleBatchProcessor\n\n\nclass MyBatchProcessor(SimpleBatchProcessor):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.criterion = torch.nn.CrossEntropyLoss()  # Define your loss\n\n    def forward(\n        self,\n        model: torch.nn.Module,\n        batch: Tuple[torch.Tensor, torch.Tensor],\n    ) -> Tuple[Any, Optional[torch.Tensor]]:\n        # unpack batch by yourself\n        images, target = batch\n\n        # Accelerator has already moved batch to the correct device\n        output = model(images)\n        loss = self.criterion(output, target) if self.need_backward else None\n\n        return output, loss  # Returns model output and loss\n\n\nfrom torch.optim import SGD\nfrom torch.optim.lr_scheduler import StepLR\n\ntrain_dataset, _ = cfg.dataset.get_dataset()\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=cfg.batch_size,\n    shuffle=True,\n    num_workers=cfg.num_workers,\n    pin_memory=False,\n)\n\nmodel = models.resnet50()\n\noptimizer = SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\nlr_scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n\nhooks = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core concepts of the hook System\n\n* :py:class:`~robo_orchard_lab.pipeline.hooks.mixin.PipelineHookArgs`: A dataclass holding all relevant information (accelerator,\n  epoch/step IDs, batch data, model outputs, loss, etc.) passed to each hook.\n  This ensures hooks have a standardized, type-safe context.\n\n* :py:class:`~robo_orchard_lab.pipeline.hooks.mixin.PipelineHooksConfig`: The entire :py:class:`~robo_orchard_lab.pipeline.hooks.mixin.PipelineHooks` setup,\n  including which individual hooks are active and their parameters, can often be defined via Pydantic configurations.\n  This offers great flexibility and reproducibility. For instance, a main\n  experiment config might point to a :py:class:`~robo_orchard_lab.pipeline.hooks.mixin.PipelineHooksConfig`,\n  which in turn might specify a list of individual hook configurations (e.g.,\n  :py:class:`~robo_orchard_lab.pipeline.hooks.metric.MetricTrackerConfig`,\n  :py:class:`~robo_orchard_lab.pipeline.hooks.checkpoint.SaveCheckpointConfig`).\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Some built-in hooks\n\nTo streamline common training tasks and reduce boilerplate code, **robo_orchard_lab**\nprovides a collection of pre-built hooks. These hooks are implemented using the same\nunderlying :py:class:`~robo_orchard_lab.pipeline.hooks.mixin.PipelineHooks` and\n:py:class:`~robo_orchard_lab.pipeline.hooks.mixin.HookContext` mechanisms discussed\npreviously.\n\nTypically, you interact with these built-in hooks by instantiating their\ncorresponding Pydantic configuration classes (e.g., ``MetricTrackerConfig``,\n``StatsMonitorConfig``, ``SaveCheckpointConfig``). These config objects allow you\nto customize the behavior of the hook and are then used to create the actual\nhook instances, which are subsequently added to your main ``HookBasedTrainer``'s\nlist of active hooks.\n\nIn the following subsections, we will explore how to configure and use some of\nthe most common built-in hooks provided by the framework. While these cover\nfrequent use cases, remember that you can always create your own custom hooks\n(as shown in the \"(Advanced) Creating Your First Custom Hook\" section) for more\nspecialized requirements.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MetricTracker: Track on performance\n:py:class:`~robo_orchard_lab.pipeline.hooks.metric.MetricTracker` is a specialized hook for handling metrics.\nIt takes a list of :py:class:`~robo_orchard_lab.pipeline.hooks.metric.MetricEntry` objects. Each MetricEntry defines:\n\n* names: How the metric will be logged.\n\n* metric: An instance of a torchmetrics metric (or any compatible metric object).\n\n:py:class:`~robo_orchard_lab.pipeline.hooks.metric.MetricTracker` is an abstarct class, you should inherit it\nand implement the :py:meth:`~robo_orchard_lab.pipeline.hooks.metric.MetricTracker.update_metric` method, which is called by the trainer to update these metrics with batch outputs and targets.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchmetrics import Accuracy as AccuracyMetric\n\nfrom robo_orchard_lab.pipeline.hooks import (\n    MetricEntry,\n    MetricTracker,\n    MetricTrackerConfig,\n)\n\n\nclass MyMetricTracker(MetricTracker):\n    def update_metric(self, batch: Any, model_outputs: Any):\n        _, targets = batch\n        for metric_i in self.metrics:\n            metric_i(model_outputs, targets)\n\n\nclass MyMetricTrackerConfig(MetricTrackerConfig):\n    \"\"\"An example metric tracker config.\"\"\"\n\n    # note: bind MyMetricTracker\n    class_type: type[MyMetricTracker] = MyMetricTracker\n\n\nmetric_tracker = MyMetricTrackerConfig(\n    metric_entrys=[\n        MetricEntry(\n            names=[\"top1_acc\"],\n            metric=AccuracyMetric(\n                task=\"multiclass\", num_classes=1000, top_k=1\n            ),\n        ),\n        MetricEntry(\n            names=[\"top5_acc\"],\n            metric=AccuracyMetric(\n                task=\"multiclass\", num_classes=1000, top_k=5\n            ),\n        ),\n    ],\n    step_log_freq=64,\n    log_main_process_only=False,\n)\n\nhooks.append(metric_tracker)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### StatsMonitor: Logging Training Vitals\n\n:py:class:`~robo_orchard_lab.pipeline.hooks.stats.StatsMonitor` monitors and logs statistics like learning rate, training speed (samples/sec), estimated time remaining, etc.\nIts ``step_log_freq`` controls how often this information is printed or logged.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from robo_orchard_lab.pipeline.hooks import StatsMonitorConfig\n\nstats = StatsMonitorConfig(step_log_freq=64)\n\nhooks.append(stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SaveCheckpoint: Saving Your Progress\n\n:py:class:`~robo_orchard_lab.pipeline.hooks.checkpoint.SaveCheckpointConfig` is responsible for triggering model checkpoint saves.\nIt calls :py:class:`~acclerator.Accelerator.save_state()`` internally. ``save_step_freq`` defines how many training steps between checkpoints.\nResuming is handled by Accelerator\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from robo_orchard_lab.pipeline.hooks import SaveCheckpointConfig\n\nsave_checkpoint = SaveCheckpointConfig(save_step_freq=1024)\n\nhooks.append(save_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Orchestrating the Training\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from robo_orchard_lab.pipeline import HookBasedTrainer\n\ntrainer = HookBasedTrainer(\n    model=model,\n    dataloader=train_dataloader,\n    optimizer=optimizer,\n    lr_scheduler=lr_scheduler,\n    accelerator=accelerator,\n    batch_processor=MyBatchProcessor(need_backward=True),\n    max_epoch=cfg.max_epoch,\n    hooks=hooks,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Show hooks\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(trainer.hooks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Begin training\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All the checkpoints is saved to ``cfg.workspace``\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import subprocess\n\nprint(subprocess.check_output([\"tree\", cfg.workspace_root]).decode())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}